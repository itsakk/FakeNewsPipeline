{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f01c68b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31353c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politifact11773\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n",
      "sleeping for 15 minutes\n"
     ]
    }
   ],
   "source": [
    "def auth():\n",
    "    bearer_token = \"AAAAAAAAAAAAAAAAAAAAAPtlMQEAAAAAYuHQKOkS6s%2BfFNrKLOqKGb3wwLA%3Dlxl5JY3It9kNqcctounHqi3heHmJDtPBJ4v6sepKebMSMRdaG6\"\n",
    "    return bearer_token\n",
    "\n",
    "def get_news_files(news_source, label):\n",
    "    path = 'C:\\\\Users\\\\arman\\\\Documents\\\\Bristol_DataScience_MSc\\\\MSc_Thesis\\\\FakeNewsNet-master\\\\code\\\\fakenewsnet_dataset\\\\'+news_source+'\\\\'+label\n",
    "    os.chdir(path)\n",
    "    list_news_files = os.listdir(path)\n",
    "    return list_news_files\n",
    "\n",
    "def get_ids(news_source, filename, label):\n",
    "    path = 'C:\\\\Users\\\\arman\\\\Documents\\\\Bristol_DataScience_MSc\\\\MSc_Thesis\\\\FakeNewsNet-master\\\\code\\\\fakenewsnet_dataset\\\\'+news_source+'\\\\'+label+'\\\\'+filename+'\\\\tweets'\n",
    "    os.chdir(path)\n",
    "    list_tweets = os.listdir(path)\n",
    "    user_ids = []\n",
    "    for tweet_id in list_tweets:\n",
    "        f = open(tweet_id,)\n",
    "        tweet = json.load(f)\n",
    "        user_ids.append(str(tweet['user']['id']))\n",
    "    return user_ids, list_tweets\n",
    "\n",
    "def create_url(user_id):\n",
    "    url = 'https://api.twitter.com/2/users/{}/tweets?expansions=author_id&max_results=100'.format(user_id)\n",
    "    return url\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "def connect_to_endpoint(url, headers):\n",
    "    connection_timeout = 30\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.request(\"GET\", url, headers=headers)\n",
    "            \n",
    "            if response.status_code == 403:\n",
    "                response = []\n",
    "                return response\n",
    "            \n",
    "            if response.status_code == 404:       \n",
    "                response = []\n",
    "                return response\n",
    "            \n",
    "            if response.status_code == 429:\n",
    "                print('sleeping for 15 minutes')        \n",
    "                time.sleep(900)\n",
    "                response = requests.request(\"GET\", url, headers=headers)\n",
    "\n",
    "                if response.status_code == 403:\n",
    "                    response = []\n",
    "                    return response\n",
    "\n",
    "                if response.status_code == 404:\n",
    "                    response = []\n",
    "                    return response\n",
    "            else:\n",
    "                return response.json()\n",
    "            \n",
    "        except requests.ConnectionError as ce:\n",
    "            if time.time() > start_time + connection_timeout:\n",
    "                raise ('ConnectionError: Could not connect within %s seconds')\n",
    "            else:\n",
    "                time.sleep(1)\n",
    "    \n",
    "def save_to_json_file(json_response, news_source, label, filename, tweet_id):\n",
    "    if len(json_response) !=0:\n",
    "        if not os.path.exists('C:\\\\Users\\\\arman\\\\Documents\\\\Bristol_DataScience_MSc\\\\MSc_Thesis\\\\FakeNewsNet-master\\\\code\\\\fakenewsnet_dataset\\\\'+news_source+'\\\\'+label+'\\\\'+filename+'\\\\user_timeline_tweets'):\n",
    "            os.mkdir('C:\\\\Users\\\\arman\\\\Documents\\\\Bristol_DataScience_MSc\\\\MSc_Thesis\\\\FakeNewsNet-master\\\\code\\\\fakenewsnet_dataset\\\\'+news_source+'\\\\'+label+'\\\\'+filename+'\\\\user_timeline_tweets')\n",
    "        with open('C:\\\\Users\\\\arman\\\\Documents\\\\Bristol_DataScience_MSc\\\\MSc_Thesis\\\\FakeNewsNet-master\\\\code\\\\fakenewsnet_dataset\\\\'+news_source+'\\\\'+label+'\\\\'+filename+'\\\\user_timeline_tweets\\\\'+tweet_id, 'w') as outfile:\n",
    "                        json.dump(json_response, outfile)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "def main():\n",
    "    bearer_token = auth()\n",
    "    labels = ['fake', 'real']\n",
    "    news_sources = ['politifact']\n",
    "    for source in news_sources:\n",
    "        for label in labels:\n",
    "            list_news_files = get_news_files(source, label)\n",
    "            for filename in list_news_files: #[1569:]\n",
    "                print(filename)\n",
    "                try:\n",
    "                    ids, list_tweets = get_ids(source, filename, label)\n",
    "                    for i, ID in enumerate(ids):\n",
    "                        url = create_url(ID)\n",
    "                        headers = create_headers(bearer_token)\n",
    "                        json_response = connect_to_endpoint(url, headers)\n",
    "                        save_to_json_file(json_response, source, label, filename, list_tweets[i])\n",
    "                except:\n",
    "                    pass\n",
    "            print(f'retweets related to {label} news of {source} collected')\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
